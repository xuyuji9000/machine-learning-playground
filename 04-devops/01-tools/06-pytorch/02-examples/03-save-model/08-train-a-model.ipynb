{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current example follows this [document](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare dependencies\n",
    "import mnist\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. prepare training and testing dataloader\n",
    "\n",
    "mnist.datasets_url=\"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/\"\n",
    "\n",
    "train_images = mnist.train_images()\n",
    "train_labels = mnist.train_labels()\n",
    "test_images = mnist.test_images()\n",
    "test_labels = mnist.test_labels()\n",
    "\n",
    "\n",
    "train_images_tensor = torch.Tensor(train_images)\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.int64)\n",
    "test_images_tensor = torch.Tensor(test_images)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.int64)\n",
    "\n",
    "train_dataset = TensorDataset(train_images_tensor, train_labels_tensor)\n",
    "test_dataset = TensorDataset(test_images_tensor, test_labels_tensor)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=10, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=10, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the train_loop function\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        \n",
    "        \n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f'loss: {loss:>7f} [{current:>5d}/{size:>5d}]')\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y ).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /=size\n",
    "\n",
    "    print(f'Test Error: \\n Accurary: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------\n",
      "loss: 16.390299 [   64/60000]\n",
      "loss: 2.319577 [ 6464/60000]\n",
      "loss: 2.349446 [12864/60000]\n",
      "loss: 2.290609 [19264/60000]\n",
      "loss: 2.340511 [25664/60000]\n",
      "loss: 2.343422 [32064/60000]\n",
      "loss: 2.314474 [38464/60000]\n",
      "loss: 2.355653 [44864/60000]\n",
      "loss: 2.300287 [51264/60000]\n",
      "loss: 2.276902 [57664/60000]\n",
      "Test Error: \n",
      " Accurary: 10.0%, Avg loss: 2.320933 \n",
      "\n",
      "Epoch 2\n",
      "--------------------\n",
      "loss: 2.340177 [   64/60000]\n",
      "loss: 2.285198 [ 6464/60000]\n",
      "loss: 2.311335 [12864/60000]\n",
      "loss: 2.320681 [19264/60000]\n",
      "loss: 2.317040 [25664/60000]\n",
      "loss: 2.326154 [32064/60000]\n",
      "loss: 2.318019 [38464/60000]\n",
      "loss: 2.329008 [44864/60000]\n",
      "loss: 2.210922 [51264/60000]\n",
      "loss: 2.333088 [57664/60000]\n",
      "Test Error: \n",
      " Accurary: 17.8%, Avg loss: 2.185964 \n",
      "\n",
      "Epoch 3\n",
      "--------------------\n",
      "loss: 2.474085 [   64/60000]\n",
      "loss: 2.101439 [ 6464/60000]\n",
      "loss: 2.166438 [12864/60000]\n",
      "loss: 2.236853 [19264/60000]\n",
      "loss: 2.242551 [25664/60000]\n",
      "loss: 1.951383 [32064/60000]\n",
      "loss: 2.203146 [38464/60000]\n",
      "loss: 2.139006 [44864/60000]\n",
      "loss: 2.095916 [51264/60000]\n",
      "loss: 2.175304 [57664/60000]\n",
      "Test Error: \n",
      " Accurary: 18.0%, Avg loss: 2.129758 \n",
      "\n",
      "Epoch 4\n",
      "--------------------\n",
      "loss: 2.185017 [   64/60000]\n",
      "loss: 2.004434 [ 6464/60000]\n",
      "loss: 2.053782 [12864/60000]\n",
      "loss: 2.153572 [19264/60000]\n",
      "loss: 2.052762 [25664/60000]\n",
      "loss: 1.946388 [32064/60000]\n",
      "loss: 2.145271 [38464/60000]\n",
      "loss: 2.170302 [44864/60000]\n",
      "loss: 2.230570 [51264/60000]\n",
      "loss: 2.177034 [57664/60000]\n",
      "Test Error: \n",
      " Accurary: 18.4%, Avg loss: 2.193294 \n",
      "\n",
      "Epoch 5\n",
      "--------------------\n",
      "loss: 2.021851 [   64/60000]\n",
      "loss: 2.135217 [ 6464/60000]\n",
      "loss: 2.121213 [12864/60000]\n",
      "loss: 2.179211 [19264/60000]\n",
      "loss: 2.295562 [25664/60000]\n",
      "loss: 2.101929 [32064/60000]\n",
      "loss: 2.196271 [38464/60000]\n",
      "loss: 1.978896 [44864/60000]\n",
      "loss: 2.076464 [51264/60000]\n",
      "loss: 2.175802 [57664/60000]\n",
      "Test Error: \n",
      " Accurary: 18.7%, Avg loss: 2.114061 \n",
      "\n",
      "Epoch 6\n",
      "--------------------\n",
      "loss: 2.179956 [   64/60000]\n",
      "loss: 2.098846 [ 6464/60000]\n",
      "loss: 2.068622 [12864/60000]\n",
      "loss: 2.050055 [19264/60000]\n",
      "loss: 2.033788 [25664/60000]\n",
      "loss: 2.041328 [32064/60000]\n",
      "loss: 1.960740 [38464/60000]\n",
      "loss: 2.039631 [44864/60000]\n",
      "loss: 2.030436 [51264/60000]\n",
      "loss: 2.027011 [57664/60000]\n",
      "Test Error: \n",
      " Accurary: 18.9%, Avg loss: 2.092096 \n",
      "\n",
      "Epoch 7\n",
      "--------------------\n",
      "loss: 2.176875 [   64/60000]\n",
      "loss: 2.079584 [ 6464/60000]\n",
      "loss: 1.981790 [12864/60000]\n",
      "loss: 2.128427 [19264/60000]\n",
      "loss: 2.191121 [25664/60000]\n",
      "loss: 2.163978 [32064/60000]\n",
      "loss: 2.034872 [38464/60000]\n",
      "loss: 2.085875 [44864/60000]\n",
      "loss: 2.033526 [51264/60000]\n",
      "loss: 2.087309 [57664/60000]\n",
      "Test Error: \n",
      " Accurary: 20.5%, Avg loss: 2.037661 \n",
      "\n",
      "Epoch 8\n",
      "--------------------\n",
      "loss: 1.996217 [   64/60000]\n",
      "loss: 2.033616 [ 6464/60000]\n",
      "loss: 1.876772 [12864/60000]\n",
      "loss: 1.934941 [19264/60000]\n",
      "loss: 1.878550 [25664/60000]\n",
      "loss: 2.092029 [32064/60000]\n",
      "loss: 2.025333 [38464/60000]\n",
      "loss: 1.887031 [44864/60000]\n",
      "loss: 1.929997 [51264/60000]\n",
      "loss: 1.983635 [57664/60000]\n",
      "Test Error: \n",
      " Accurary: 22.1%, Avg loss: 1.968762 \n",
      "\n",
      "Epoch 9\n",
      "--------------------\n",
      "loss: 1.876817 [   64/60000]\n",
      "loss: 1.993345 [ 6464/60000]\n",
      "loss: 1.925179 [12864/60000]\n",
      "loss: 1.759948 [19264/60000]\n",
      "loss: 1.898740 [25664/60000]\n",
      "loss: 1.864948 [32064/60000]\n",
      "loss: 1.931998 [38464/60000]\n",
      "loss: 2.163752 [44864/60000]\n",
      "loss: 1.848485 [51264/60000]\n",
      "loss: 1.831865 [57664/60000]\n",
      "Test Error: \n",
      " Accurary: 12.6%, Avg loss: 1.882928 \n",
      "\n",
      "Epoch 10\n",
      "--------------------\n",
      "loss: 1.904125 [   64/60000]\n",
      "loss: 1.990165 [ 6464/60000]\n",
      "loss: 1.869786 [12864/60000]\n",
      "loss: 1.780928 [19264/60000]\n",
      "loss: 1.611540 [25664/60000]\n",
      "loss: 1.942313 [32064/60000]\n",
      "loss: 1.854957 [38464/60000]\n",
      "loss: 1.775465 [44864/60000]\n",
      "loss: 1.891189 [51264/60000]\n",
      "loss: 1.966640 [57664/60000]\n",
      "Test Error: \n",
      " Accurary: 24.6%, Avg loss: 1.853809 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. training model\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer =torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f'Epoch {t+1}\\n--------------------')\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_relu_stack.0.weight',\n",
       "              tensor([[-1.6768e-02, -5.6354e-03, -7.6049e-03,  ...,  1.8614e-02,\n",
       "                        1.4288e-02,  4.4046e-05],\n",
       "                      [ 7.1397e-03, -2.5362e-02, -2.0448e-02,  ...,  2.6557e-02,\n",
       "                       -3.1808e-02,  3.3969e-02],\n",
       "                      [ 1.0782e-02,  5.2428e-03, -8.2378e-03,  ..., -8.2695e-03,\n",
       "                        3.3830e-02, -2.8091e-03],\n",
       "                      ...,\n",
       "                      [ 1.6517e-03, -1.6262e-02, -3.2337e-02,  ...,  7.5016e-03,\n",
       "                       -3.2627e-02,  2.1107e-02],\n",
       "                      [ 1.7445e-02, -5.3874e-03, -3.1857e-02,  ..., -4.3365e-03,\n",
       "                       -2.5193e-02, -3.0141e-02],\n",
       "                      [ 1.7908e-02,  4.9070e-03,  1.4841e-02,  ...,  2.4168e-02,\n",
       "                       -2.2036e-02, -1.0493e-03]])),\n",
       "             ('linear_relu_stack.0.bias',\n",
       "              tensor([ 4.8575e-03, -3.2404e-04, -3.0490e-02,  1.4438e-02,  2.2419e-02,\n",
       "                       1.6507e-02,  4.0741e-03,  3.4099e-02,  3.3886e-03, -8.9662e-03,\n",
       "                      -2.6724e-02, -1.3468e-02,  2.6218e-02, -5.5278e-03,  4.7783e-04,\n",
       "                       1.6730e-02, -1.6954e-02, -2.7802e-02,  1.6188e-02,  8.3095e-03,\n",
       "                      -7.4430e-03,  2.9230e-02, -1.9904e-02,  2.9597e-02, -2.3487e-02,\n",
       "                       5.2602e-03, -2.4616e-02,  2.6779e-02, -2.6433e-02,  6.6077e-03,\n",
       "                       2.9462e-02, -4.5680e-03,  2.8055e-02,  3.4139e-02, -4.4609e-03,\n",
       "                       4.1537e-03, -3.3368e-03, -2.0434e-02, -3.1515e-03, -1.6336e-02,\n",
       "                      -8.0638e-03,  1.7456e-02,  5.0151e-03,  1.9910e-02,  1.6968e-02,\n",
       "                       3.4068e-02,  1.5507e-02,  5.6913e-03, -2.1698e-03,  2.3958e-02,\n",
       "                       3.3917e-02, -1.0418e-02,  5.9978e-05,  2.2073e-02,  1.7524e-03,\n",
       "                      -4.9295e-03, -1.0354e-03, -7.1934e-03,  8.3750e-03, -1.9140e-02,\n",
       "                       1.8034e-03,  1.5632e-02, -2.1676e-02, -5.0110e-03,  2.2723e-02,\n",
       "                      -7.3558e-03, -2.9355e-02, -4.8966e-03,  3.1647e-02,  2.6774e-02,\n",
       "                      -1.4854e-03, -3.5557e-03,  3.0671e-02, -1.3734e-02, -1.8865e-02,\n",
       "                      -2.8167e-02, -2.0478e-02, -1.6789e-02,  3.5118e-02, -3.0540e-02,\n",
       "                      -2.0571e-02,  2.7050e-02, -7.1474e-03, -2.2494e-02, -1.6960e-02,\n",
       "                      -2.2553e-02,  3.4884e-02, -2.9382e-03, -6.7545e-04,  2.5911e-03,\n",
       "                       3.0167e-02,  2.0384e-02,  1.6161e-03,  5.5594e-03, -1.7078e-02,\n",
       "                      -8.4060e-03, -1.7525e-02,  9.3004e-03,  2.5950e-02,  3.1148e-02,\n",
       "                      -3.0795e-02,  2.5612e-02,  3.8675e-03,  2.9818e-02,  2.2334e-02,\n",
       "                      -1.7667e-02,  7.3596e-03, -3.3612e-02, -1.8011e-02,  2.4305e-02,\n",
       "                       1.5285e-02, -3.3687e-03,  2.0595e-02,  2.2326e-02,  2.8368e-02,\n",
       "                       2.6421e-03, -2.0463e-02,  1.7167e-03, -4.5532e-03, -3.3370e-02,\n",
       "                      -2.0976e-02,  2.5545e-02,  2.7706e-02, -1.7626e-02,  1.2205e-02,\n",
       "                      -1.2314e-02, -1.3014e-02,  5.4832e-03, -1.5506e-02, -3.3709e-02,\n",
       "                      -3.5456e-03,  8.7670e-03, -8.3865e-03, -7.1819e-03,  1.3083e-02,\n",
       "                       7.2134e-03, -1.9158e-02, -2.6854e-02, -3.1869e-02, -1.8353e-02,\n",
       "                      -1.4323e-02, -2.7220e-03, -1.4616e-02, -1.0745e-03, -3.4263e-02,\n",
       "                       2.8815e-02,  1.4642e-03,  2.8100e-02, -3.2955e-02, -2.1958e-02,\n",
       "                      -5.7157e-03, -9.2127e-03,  3.0272e-02,  7.8606e-03,  2.9356e-02,\n",
       "                      -2.2867e-02,  3.4913e-02, -1.3688e-02,  2.9613e-02, -1.1575e-02,\n",
       "                       1.1732e-02,  1.8860e-02,  1.1548e-02,  6.8295e-03, -1.4530e-02,\n",
       "                      -1.1878e-02, -1.5377e-02, -9.9278e-03, -2.1437e-03, -4.1047e-03,\n",
       "                      -2.3145e-02,  3.5022e-02,  2.5361e-02, -4.4129e-03,  3.3461e-02,\n",
       "                      -1.2500e-02,  2.0274e-02,  2.9103e-02,  6.1194e-03, -2.6251e-02,\n",
       "                       2.4480e-02,  1.6097e-02,  3.4023e-02, -2.4296e-02, -1.0968e-02,\n",
       "                      -3.2848e-02,  3.4486e-02, -1.5169e-03, -2.8764e-02, -2.5985e-02,\n",
       "                       1.0268e-02,  2.0726e-03,  1.1428e-02,  1.6869e-03,  1.1027e-03,\n",
       "                       2.4619e-02,  2.1711e-02,  5.0639e-03, -2.8601e-02, -1.7905e-02,\n",
       "                      -5.5000e-03, -3.0629e-02, -9.9321e-03, -1.0384e-02,  2.3361e-02,\n",
       "                      -3.1124e-02, -4.8939e-03, -2.5602e-02,  2.2475e-02,  1.5241e-02,\n",
       "                       3.4051e-02, -1.5004e-02,  2.9697e-02, -2.2739e-02,  3.1884e-02,\n",
       "                      -1.7801e-03, -1.0171e-02, -2.1617e-02, -9.4015e-03, -2.1894e-03,\n",
       "                       1.4478e-02, -2.8852e-02, -2.8071e-02,  3.5169e-02, -1.3237e-02,\n",
       "                      -6.1284e-03,  5.1585e-03, -1.7216e-02, -1.5348e-03,  1.5091e-03,\n",
       "                      -2.8484e-02, -8.8667e-03,  1.7117e-02, -1.4038e-03, -9.4208e-03,\n",
       "                      -2.6553e-02, -1.7765e-02,  6.5328e-03, -3.4396e-02,  2.1596e-02,\n",
       "                      -2.6408e-02,  7.2948e-03,  6.6188e-03,  3.5413e-02,  1.4276e-02,\n",
       "                       2.6197e-02,  3.1327e-02,  3.5326e-02,  2.2952e-02,  1.9322e-03,\n",
       "                      -1.5903e-03, -9.1146e-03, -1.2704e-02,  4.0469e-03,  3.1589e-02,\n",
       "                       1.3475e-02,  3.2333e-03,  1.1773e-02, -1.5457e-02,  7.9298e-03,\n",
       "                      -2.3559e-02,  2.9666e-02, -2.6213e-02,  2.4648e-02,  4.0836e-03,\n",
       "                       2.0355e-02,  2.4041e-02,  9.2005e-03,  1.2948e-02,  5.0799e-03,\n",
       "                       9.0395e-03,  2.6613e-02, -2.6483e-02,  3.4914e-02, -1.0268e-02,\n",
       "                       2.9846e-02,  1.8458e-03,  1.4459e-02, -3.0746e-02, -2.6198e-03,\n",
       "                      -5.2302e-03, -1.8146e-02,  2.1339e-02,  2.7023e-02, -1.7941e-02,\n",
       "                      -1.7803e-02, -2.3571e-02,  2.2509e-02,  1.7903e-02,  1.8734e-02,\n",
       "                       1.3564e-02,  3.1213e-03, -2.3858e-02, -2.3339e-02, -2.8246e-02,\n",
       "                       3.0741e-02,  2.6670e-02, -2.6028e-02, -2.7541e-02, -3.0203e-02,\n",
       "                       3.3083e-02,  2.9243e-02, -2.8762e-02, -8.1175e-03,  6.4995e-03,\n",
       "                       7.3267e-03,  2.8170e-02, -3.4976e-02,  1.1527e-02,  3.1102e-02,\n",
       "                      -1.1564e-03, -1.6095e-02,  9.5600e-03, -2.2310e-02,  9.2377e-03,\n",
       "                      -2.1040e-02, -5.1737e-03, -3.0298e-02,  1.9937e-02, -1.1179e-02,\n",
       "                       1.5958e-02,  2.9096e-02,  1.0067e-02, -9.9664e-03, -6.8324e-03,\n",
       "                      -3.0181e-03, -3.1791e-02,  3.4764e-02, -2.6158e-03, -1.4413e-02,\n",
       "                       6.9822e-03, -9.0187e-03, -2.4560e-02,  1.4533e-02,  6.2196e-04,\n",
       "                      -3.1824e-02, -3.2854e-02,  1.4620e-02, -3.5057e-02,  9.6862e-03,\n",
       "                      -3.4872e-02, -6.1447e-03,  1.4934e-02, -1.6864e-02,  3.5161e-02,\n",
       "                      -2.8026e-02, -2.3661e-02, -3.1927e-02, -3.2957e-02, -3.0677e-02,\n",
       "                       7.4038e-03,  3.3883e-02, -2.1543e-02, -2.5872e-02,  3.3910e-02,\n",
       "                      -4.1348e-03,  1.7109e-03, -1.3201e-02,  3.2394e-02,  2.8074e-02,\n",
       "                      -1.8842e-02,  1.9449e-02,  2.2377e-02,  2.3588e-02,  2.7138e-02,\n",
       "                       3.2883e-02, -3.2794e-03,  1.3504e-02, -2.1994e-02, -1.6452e-02,\n",
       "                      -1.5037e-03, -3.4008e-02, -3.2747e-02, -1.4186e-02,  2.1879e-02,\n",
       "                      -2.8081e-02, -3.1405e-02,  1.8232e-03, -2.7246e-02, -1.4501e-02,\n",
       "                      -2.2907e-02, -3.1927e-02,  2.1561e-02,  3.2684e-02,  9.3768e-03,\n",
       "                       2.3759e-02, -6.2776e-03, -1.8984e-04, -6.2319e-03, -1.2502e-02,\n",
       "                       3.4861e-02,  1.0456e-02, -2.3082e-02, -2.5478e-02,  3.0415e-03,\n",
       "                      -2.9080e-03, -3.4104e-02,  1.7440e-02, -1.1785e-02,  1.9251e-02,\n",
       "                      -2.3827e-02, -6.5911e-03,  1.3683e-02,  4.8914e-03,  2.1345e-02,\n",
       "                       1.0825e-02, -3.5192e-02, -2.7984e-02,  5.8628e-03, -3.2299e-02,\n",
       "                       2.3023e-02,  2.3063e-02, -1.8435e-02,  3.0997e-03, -5.0050e-03,\n",
       "                       1.1989e-02, -3.5269e-02,  3.0502e-02,  5.4672e-03, -2.2984e-02,\n",
       "                      -3.2436e-02, -1.1225e-03, -2.4044e-02, -1.7657e-02, -2.9429e-03,\n",
       "                      -2.7339e-02,  9.6736e-03,  5.9482e-03,  4.0210e-03,  2.5260e-02,\n",
       "                      -2.7451e-02, -2.0189e-02,  2.0483e-02,  2.6259e-02,  3.0505e-02,\n",
       "                      -1.5696e-02, -4.4414e-03,  1.4801e-02, -2.3790e-03, -2.7699e-02,\n",
       "                       3.1051e-02, -8.1097e-03,  1.8007e-02,  1.5947e-02,  3.9135e-03,\n",
       "                      -1.9903e-02,  3.4422e-02,  1.4152e-02, -2.0043e-02,  5.0451e-03,\n",
       "                      -3.5190e-02, -1.6085e-02,  2.3869e-02,  4.6109e-03,  2.3607e-02,\n",
       "                      -9.2128e-03, -1.0738e-02,  5.4228e-03, -2.0422e-02, -1.6948e-02,\n",
       "                      -9.1758e-03,  1.5087e-02,  3.1876e-02,  2.3629e-02, -2.8507e-02,\n",
       "                       3.6109e-03,  7.3388e-03, -1.0154e-02,  1.3042e-02, -2.8689e-02,\n",
       "                      -2.7816e-02,  4.1549e-03, -1.8690e-02, -1.0244e-02,  8.9899e-03,\n",
       "                      -4.0389e-03,  4.4172e-03, -1.7397e-02, -3.0846e-02, -1.6799e-02,\n",
       "                       5.3028e-03, -3.0795e-02, -7.5615e-03,  3.3643e-02, -1.0055e-02,\n",
       "                      -1.9971e-02,  6.9016e-03,  7.0433e-03, -3.1214e-03, -2.4146e-02,\n",
       "                       2.3760e-02, -1.0550e-02,  5.0953e-03,  1.5476e-02, -2.9580e-02,\n",
       "                      -4.7600e-03, -2.4427e-02,  1.9046e-02,  3.1892e-02, -2.4766e-02,\n",
       "                       3.2926e-02,  1.1894e-02, -3.5972e-05, -1.5198e-02,  2.5572e-02,\n",
       "                      -3.2407e-02,  2.8097e-02,  1.6032e-02,  9.3751e-03,  5.7217e-04,\n",
       "                      -2.1874e-02, -3.1209e-02])),\n",
       "             ('linear_relu_stack.2.weight',\n",
       "              tensor([[ 0.0008,  0.0021,  0.0114,  ...,  0.0090,  0.0082, -0.0181],\n",
       "                      [-0.0243,  0.0159, -0.0082,  ..., -0.0265, -0.0304, -0.0346],\n",
       "                      [-0.0264, -0.0269, -0.0412,  ...,  0.0088, -0.0227,  0.0033],\n",
       "                      ...,\n",
       "                      [ 0.0108,  0.0251, -0.0378,  ...,  0.0092,  0.0019, -0.0020],\n",
       "                      [ 0.0359,  0.0374,  0.0334,  ..., -0.0334, -0.0246, -0.0128],\n",
       "                      [-0.0179,  0.0271,  0.0273,  ..., -0.0514,  0.0320, -0.0238]])),\n",
       "             ('linear_relu_stack.2.bias',\n",
       "              tensor([-0.0305,  0.0171, -0.0260, -0.0176, -0.0161,  0.0222, -0.0280,  0.0068,\n",
       "                       0.0150,  0.0017])),\n",
       "             ('linear_relu_stack.4.weight',\n",
       "              tensor([[-0.1136, -0.2134, -0.1603,  0.1929, -0.2381,  0.1427, -0.0886, -0.0864,\n",
       "                       -0.1045,  0.1782],\n",
       "                      [ 0.1027, -0.1051,  0.1977,  0.1777,  0.2194, -0.1199,  0.0420, -0.1829,\n",
       "                        0.2131,  0.1581],\n",
       "                      [-0.3025,  0.2793, -0.3091,  0.2257, -0.0534,  0.0968,  0.1265, -0.1913,\n",
       "                        0.2353,  0.3001],\n",
       "                      [ 0.1048, -0.1904, -0.1411, -0.0329, -0.1943, -0.1640,  0.0051, -0.3207,\n",
       "                        0.0758,  0.1676],\n",
       "                      [-0.0862,  0.2637, -0.0704,  0.1122,  0.0760, -0.2809, -0.2181, -0.1351,\n",
       "                        0.1868, -0.0541],\n",
       "                      [ 0.0124, -0.2798, -0.1043,  0.1049, -0.1187, -0.2537,  0.1801,  0.1536,\n",
       "                       -0.0209, -0.0366],\n",
       "                      [-0.1576, -0.0396, -0.1654, -0.2989,  0.1804,  0.0263, -0.3049, -0.1322,\n",
       "                       -0.0672,  0.0115],\n",
       "                      [ 0.0039, -0.3135, -0.2097, -0.2678, -0.0952, -0.2715, -0.2932,  0.1790,\n",
       "                        0.2353,  0.0615],\n",
       "                      [-0.0659, -0.0481, -0.1371,  0.0363,  0.1619, -0.0340, -0.3024, -0.2705,\n",
       "                        0.0452,  0.3108],\n",
       "                      [ 0.0537, -0.3091,  0.2607,  0.0249,  0.0269,  0.1972,  0.2259,  0.1396,\n",
       "                       -0.2703, -0.0486]])),\n",
       "             ('linear_relu_stack.4.bias',\n",
       "              tensor([ 0.1454, -0.0306,  0.0814, -0.0439,  0.1654, -0.0271,  0.0788, -0.6633,\n",
       "                       0.1033,  0.1443]))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# model.state_dict()\n",
    "\n",
    "torch.save(model.state_dict(), './model/main.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
